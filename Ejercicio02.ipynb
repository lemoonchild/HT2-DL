{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc2e06a",
   "metadata": {},
   "source": [
    "### Ejercicio 02 - Repaso Teoría "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c42245",
   "metadata": {},
   "source": [
    "#### 1. ¿Cuál es la principal innovación de la arquitectura Transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7124d3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "La principal innovación de la arquitectura Transformer es la eliminación total de las redes neuronales recurrentes (RNNs) y convolucionales (CNNs) en favor de un mecanismo llamado autoatención (self-attention).\n",
    "\n",
    "A diferencia de las arquitecturas anteriores que procesaban secuencias de manera secuencial (como las RNN) o con ventanas locales (como las CNN), el Transformer procesa todos los elementos de la secuencia en paralelo, permitiendo que cada posición se relacione directamente con cualquier otra, sin importar la distancia entre ellas.\n",
    "\n",
    "Esto se logra a través de mecanismos de atención escalada y multi-cabezal (multi-head attention), que permiten al modelo \"enfocarse\" en diferentes partes de la entrada simultáneamente y aprender relaciones complejas en la secuencia.\n",
    "\n",
    "Gracias a esta innovación:\n",
    "\n",
    "- Se mejora significativamente la eficiencia en el entrenamiento, aprovechando mejor el paralelismo del hardware moderno (como las GPUs).\n",
    "- Se facilita el aprendizaje de dependencias a largo plazo entre elementos de una secuencia.\n",
    "- Se logra una mayor calidad en tareas de traducción automática y otros problemas de procesamiento de lenguaje natural.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83eca6",
   "metadata": {},
   "source": [
    "#### 2. ¿Cómo funciona el mecanismo de atención del scaled dot-product?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f5ec4",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "El mecanismo de atención Scaled Dot-Product es una técnica utilizada en el modelo Transformer para **calcular la importancia relativa entre palabras de una secuencia**, tanto en la entrada como en la salida.\n",
    "\n",
    "Este mecanismo recibe tres vectores:\n",
    "\n",
    "- Q (Query): la palabra que está \"consultando\" información.\n",
    "- K (Key): las palabras candidatas a ser relevantes.\n",
    "- V (Value): la información que se va a recuperar si la palabra es relevante.\n",
    "\n",
    "El funcionamiento es el siguiente:\n",
    "\n",
    "- Se calcula la similitud entre la palabra en consulta (Query) y todas las palabras candidatas (Keys) mediante el producto punto. Esto da un valor alto si son similares.\n",
    "- El resultado se divide por la raíz cuadrada de la dimensión del vector de las claves (dk), para evitar que los valores sean muy grandes y saturen la función softmax. Esta es la parte de “scaled”.\n",
    "- Se convierte el resultado en una distribución de probabilidad, asignando mayor peso a las palabras más relevantes.\n",
    "- Se usa esa distribución para hacer un promedio ponderado de los vectores de valor (Values), resultando en un nuevo vector que representa la información contextual para la palabra original.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a80d29",
   "metadata": {},
   "source": [
    "#### 3. ¿Por qué se utiliza la atención de múltiples cabezales en Transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88327bd6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "El modelo Transformer utiliza atención de múltiples cabezales (multi-head attention) para que el modelo **pueda capturar diferentes tipos de relaciones entre las palabras de una secuencia de manera simultánea.**\n",
    "\n",
    "En lugar de aplicar una única atención, el mecanismo multi-head divide los vectores de entrada (queries, keys y values) en varias subdimensiones más pequeñas y aplica el mecanismo de atención en paralelo sobre cada una. Cada una de estas \"cabezas\" aprende a enfocarse en distintos aspectos de la información.\n",
    "\n",
    "Después de procesar la atención en paralelo, los resultados de todas las cabezas se concatenan y se proyectan de nuevo a la dimensión original.\n",
    "\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
