{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc2e06a",
   "metadata": {},
   "source": [
    "### Ejercicio 02 - Repaso Teoría "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c42245",
   "metadata": {},
   "source": [
    "#### 1. ¿Cuál es la principal innovación de la arquitectura Transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7124d3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "La principal innovación de la arquitectura Transformer es la eliminación total de las redes neuronales recurrentes (RNNs) y convolucionales (CNNs) en favor de un mecanismo llamado autoatención (self-attention).\n",
    "\n",
    "A diferencia de las arquitecturas anteriores que procesaban secuencias de manera secuencial (como las RNN) o con ventanas locales (como las CNN), el Transformer procesa todos los elementos de la secuencia en paralelo, permitiendo que cada posición se relacione directamente con cualquier otra, sin importar la distancia entre ellas.\n",
    "\n",
    "Esto se logra a través de mecanismos de atención escalada y multi-cabezal (multi-head attention), que permiten al modelo \"enfocarse\" en diferentes partes de la entrada simultáneamente y aprender relaciones complejas en la secuencia.\n",
    "\n",
    "Gracias a esta innovación:\n",
    "\n",
    "- Se mejora significativamente la eficiencia en el entrenamiento, aprovechando mejor el paralelismo del hardware moderno (como las GPUs).\n",
    "- Se facilita el aprendizaje de dependencias a largo plazo entre elementos de una secuencia.\n",
    "- Se logra una mayor calidad en tareas de traducción automática y otros problemas de procesamiento de lenguaje natural.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83eca6",
   "metadata": {},
   "source": [
    "#### 2. ¿Cómo funciona el mecanismo de atención del scaled dot-product?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f5ec4",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "El mecanismo de atención Scaled Dot-Product es una técnica utilizada en el modelo Transformer para **calcular la importancia relativa entre palabras de una secuencia**, tanto en la entrada como en la salida.\n",
    "\n",
    "Este mecanismo recibe tres vectores:\n",
    "\n",
    "- Q (Query): la palabra que está \"consultando\" información.\n",
    "- K (Key): las palabras candidatas a ser relevantes.\n",
    "- V (Value): la información que se va a recuperar si la palabra es relevante.\n",
    "\n",
    "El funcionamiento es el siguiente:\n",
    "\n",
    "- Se calcula la similitud entre la palabra en consulta (Query) y todas las palabras candidatas (Keys) mediante el producto punto. Esto da un valor alto si son similares.\n",
    "- El resultado se divide por la raíz cuadrada de la dimensión del vector de las claves (dk), para evitar que los valores sean muy grandes y saturen la función softmax. Esta es la parte de “scaled”.\n",
    "- Se convierte el resultado en una distribución de probabilidad, asignando mayor peso a las palabras más relevantes.\n",
    "- Se usa esa distribución para hacer un promedio ponderado de los vectores de valor (Values), resultando en un nuevo vector que representa la información contextual para la palabra original.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a80d29",
   "metadata": {},
   "source": [
    "#### 3. ¿Por qué se utiliza la atención de múltiples cabezales en Transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88327bd6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "El modelo Transformer utiliza atención de múltiples cabezales (multi-head attention) para que el modelo **pueda capturar diferentes tipos de relaciones entre las palabras de una secuencia de manera simultánea.**\n",
    "\n",
    "En lugar de aplicar una única atención, el mecanismo multi-head divide los vectores de entrada (queries, keys y values) en varias subdimensiones más pequeñas y aplica el mecanismo de atención en paralelo sobre cada una. Cada una de estas \"cabezas\" aprende a enfocarse en distintos aspectos de la información.\n",
    "\n",
    "Después de procesar la atención en paralelo, los resultados de todas las cabezas se concatenan y se proyectan de nuevo a la dimensión original.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dbebef",
   "metadata": {},
   "source": [
    "#### 4. ¿Cómo se incorporan los positional encodings en el modelo Transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85642e41",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "Dado que el modelo Transformer **no utiliza estructuras secuenciales** como RNNs ni convoluciones (que procesan datos con orden implícito), necesita una forma explícita de incorporar el orden de las palabras en una secuencia. Para eso, se introducen los **positional encodings** o **codificaciones posicionales**.\n",
    "\n",
    "Estos *positional encodings* son vectores que contienen información sobre la posición de cada palabra en la secuencia. Se **suman directamente a los vectores de embedding** de las palabras al inicio del encoder y del decoder. Esto permite que el modelo tenga acceso tanto al contenido semántico de cada palabra (embedding) como a su posición relativa o absoluta (codificación posicional).\n",
    "\n",
    "El paper original propone usar una función sinusoidal fija:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE(pos, 2i) &= \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\\\\n",
    "PE(pos, 2i+1) &= \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Donde `pos` es la posición en la secuencia y `i` es la dimensión del vector. Esta fórmula genera patrones repetitivos que el modelo puede usar para inferir **relaciones relativas entre posiciones**, como distancias o alineaciones.\n",
    "\n",
    "También se experimentó con **positional embeddings aprendidos**, pero los resultados fueron similares. La versión sinusoidal tiene la ventaja de **generalizar mejor a secuencias más largas** que las vistas durante el entrenamiento.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a1beb",
   "metadata": {},
   "source": [
    "#### 5. ¿Cuáles son algunas aplicaciones de la arquitectura Transformer más allá de la machine translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32e0c6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "Aunque el Transformer fue diseñado inicialmente para **traducción automática de texto**, su arquitectura ha demostrado ser altamente flexible y poderosa, y ha sido adaptada a muchas otras tareas, tanto dentro como fuera del procesamiento de lenguaje natural. Algunas aplicaciones destacadas incluyen:\n",
    "\n",
    "#### Procesamiento de lenguaje natural (NLP):\n",
    "\n",
    "* **Modelos de lenguaje** como BERT, GPT, T5 y RoBERTa, usados para tareas como:\n",
    "\n",
    "  * Clasificación de texto\n",
    "  * Resumen automático\n",
    "  * Respuesta a preguntas\n",
    "  * Detección de sentimientos\n",
    "  * Chatbots y asistentes conversacionales\n",
    "\n",
    "#### Visión por computadora:\n",
    "\n",
    "* **Vision Transformers (ViT)** han demostrado que esta arquitectura también funciona bien en imágenes, alcanzando resultados competitivos en:\n",
    "\n",
    "  * Clasificación de imágenes\n",
    "  * Detección de objetos\n",
    "  * Segmentación semántica\n",
    "\n",
    "#### Audio y habla:\n",
    "\n",
    "* Aplicaciones en procesamiento de audio como:\n",
    "\n",
    "  * Reconocimiento automático de voz (ASR)\n",
    "  * Síntesis de voz\n",
    "  * Modelos de música generativa\n",
    "\n",
    "#### Bioinformática y genómica:\n",
    "\n",
    "* Modelos como AlphaFold utilizan variantes de Transformers para **predecir estructuras de proteínas** basadas en secuencias de aminoácidos.\n",
    "\n",
    "#### Código y programación:\n",
    "\n",
    "* Modelos como **Codex** y **CodeBERT** aplican Transformers para:\n",
    "\n",
    "  * Generación automática de código\n",
    "  * Autocompletado de funciones\n",
    "  * Detección de errores o vulnerabilidades\n",
    "\n",
    "> Con esto queda más que claro que el Transformer no solo revolucionó la traducción automática, sino que se convirtió en la base de los modelos más avanzados en **NLP, visión, audio, ciencia y programación**.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdec4d0",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "\n",
    "\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
